{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb16ef61",
   "metadata": {},
   "source": [
    "# Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d473d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:29.443215Z",
     "start_time": "2022-05-16T18:25:29.419228Z"
    }
   },
   "outputs": [],
   "source": [
    "# -------- outp_version\n",
    "output_version = 2\n",
    "\n",
    "# -------- dataset\n",
    "# software_name = \"camel\"\n",
    "# software_name = \"cloudstack\"\n",
    "# software_name = \"geode\"\n",
    "software_name = \"hbase\"\n",
    "\n",
    "token_threshold = 20000\n",
    "\n",
    "# -------- bad smell\n",
    "# bad_smell = \"CC\" # Cyclomatic Complexity\n",
    "bad_smell = \"DE\" # Design\n",
    "# bad_smell = \"NC\" # Npath Complexity\n",
    "\n",
    "# --------\n",
    "my_keyword_Based = True\n",
    "my_docMaxLen = 100 if my_keyword_Based else None\n",
    "\n",
    "my_balance_train = True\n",
    "my_batch_size = 32\n",
    "my_conv_kernel_output_channel = 64\n",
    "\n",
    "# -------- google colab\n",
    "# on_google_colab = True\n",
    "on_google_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60798d75",
   "metadata": {},
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf68573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:29.459205Z",
     "start_time": "2022-05-16T18:25:29.449220Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_folder = software_name + \"_\" + bad_smell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ec7eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:29.474196Z",
     "start_time": "2022-05-16T18:25:29.463204Z"
    }
   },
   "outputs": [],
   "source": [
    "if software_name == \"camel\":\n",
    "    dataset_file_name = \"camel_DE - v.02\"\n",
    "    \n",
    "elif software_name == \"cloudstack\":\n",
    "    dataset_file_name = \"cloudstack_DE - v.01\"\n",
    "    \n",
    "elif software_name == \"geode\":\n",
    "    dataset_file_name = \"geode_DE - v.01\"\n",
    "    \n",
    "else:\n",
    "    dataset_file_name = \"hbase_DE - v.01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae99779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:29.505181Z",
     "start_time": "2022-05-16T18:25:29.480193Z"
    }
   },
   "outputs": [],
   "source": [
    "if on_google_colab:\n",
    "    \n",
    "    !pip install enlighten\n",
    "    !pip install --upgrade matplotlib\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download(\"stopwords\")\n",
    "    \n",
    "    # load data from google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "    !ls \"/content/gdrive/My Drive/\"\n",
    "    \n",
    "    tempPre = \"gdrive/MyDrive/Colab Notebooks/paper/\"\n",
    "\n",
    "else:\n",
    "    tempPre = \"E:/darsy/00/02- arshad/10- paper code/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5ba72d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:29.535162Z",
     "start_time": "2022-05-16T18:25:29.512177Z"
    }
   },
   "outputs": [],
   "source": [
    "tempData =   \"00- My Data/Datasets/Direct Method/\"\n",
    "tempOutput = \"01- Jupyter Notebook/Direct Method/00. Output/\"\n",
    "\n",
    "pre_path_data   = tempPre + tempData   + software_name + \"/\" + sub_folder + \"/\"\n",
    "pre_path_output = tempPre + tempOutput + software_name + \"/\" + sub_folder + \"/\" + dataset_file_name + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06904c1",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5844ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:39.708014Z",
     "start_time": "2022-05-16T18:25:29.540159Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import enlighten\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from types import MethodType\n",
    "from operator import truediv\n",
    "\n",
    "from torchvision import transforms\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.text import TextCollection\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0015ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:39.724005Z",
     "start_time": "2022-05-16T18:25:39.711010Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e089bf03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:39.832451Z",
     "start_time": "2022-05-16T18:25:39.729000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\n",
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "!python -V\n",
    "import matplotlib\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbf474e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:39.847447Z",
     "start_time": "2022-05-16T18:25:39.836453Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d03d3d",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a124d3bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:39.878410Z",
     "start_time": "2022-05-16T18:25:39.851444Z"
    }
   },
   "outputs": [],
   "source": [
    "mypaths = {\n",
    "    \"dataset\": pre_path_data + dataset_file_name + \".csv\",\n",
    "    \"w2v\": {\n",
    "        # \"pre_trained_model\":  pre_path_data   + \"w2vGoogle.bin\",\n",
    "        \"pre_trained_model\":  tempPre + tempData + \"w2vGoogle.bin\",\n",
    "        \"output_performance\": pre_path_output + \"performances/w2v-performance-v{}{}.json\".format(output_version, \"{}\"),\n",
    "        \"output_model\":       pre_path_output + \"models/w2v-model-v{}{}.pth\".format(output_version, \"{}\")\n",
    "    }, \n",
    "    \"tfidf\": {\n",
    "        \"output_vec\":         pre_path_output + \"tfidf-vector-v01.json\",\n",
    "        \"output_performance\": pre_path_output + \"performances/tfidf-performance-v{}{}.json\".format(output_version, \"{}\"),\n",
    "        \"output_model\":       pre_path_output + \"models/tfidf-model-v{}{}.pth\".format(output_version, \"{}\")\n",
    "    },\n",
    "    \"file_subversion\": {\n",
    "        \"im\": \".1-imbalance\",\n",
    "        \"ba\": \".2-balanced\"\n",
    "    }\n",
    "}\n",
    "\n",
    "preprocessing_params = {\n",
    "    \"columns_name\":   [\"text\",  \"bug_class_2\"],\n",
    "    \"columns_dtype\" : {0: \"str\", 1: \"int64\"},\n",
    "    \"bug_classes\": [0, 1], \n",
    "    \"num_bug_classes\": 2,\n",
    "    \"keyword_Based\": my_keyword_Based,\n",
    "    \"docMaxLen\": my_docMaxLen,\n",
    "}\n",
    "\n",
    "dataset_params = {\n",
    "    \"train_size\": 0.8,\n",
    "    \"balance_train\": my_balance_train,\n",
    "    \"batch_size\": my_batch_size\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"filter_sizes\": [3, 4, 5],\n",
    "    \"conv_kernel_output_channel\": my_conv_kernel_output_channel,\n",
    "    \"embedding_size\": 300,\n",
    "    \"longest_sentence_length\": None,\n",
    "    # probability of an element to be zeroed. Default: 0.5\n",
    "    \"dropout_zero_prob\": 0.5\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"epochs\": 20,\n",
    "    \"l2_reg_lambda\": 0.1,\n",
    "    \n",
    "    # lr params:\n",
    "    # It uses dynamic learning rate with a high value at the beginning to speed up the training\n",
    "    \"max_learning_rate\": 0.005, \n",
    "    \"min_learning_rate\": 0.0001, \n",
    "    \"decay_coefficient\": 2.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5592bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:39.893401Z",
     "start_time": "2022-05-16T18:25:39.882407Z"
    }
   },
   "outputs": [],
   "source": [
    "bcd_colours = [\"blue\", \"green\", \"red\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7071c0b",
   "metadata": {},
   "source": [
    "# Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ddd8d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:39.925235Z",
     "start_time": "2022-05-16T18:25:39.897399Z"
    }
   },
   "outputs": [],
   "source": [
    "class Rows(object):\n",
    "    def __init__(self, columns_name, bug_classes):\n",
    "        self.columns_name = columns_name\n",
    "        self.bug_classes = bug_classes\n",
    "    \n",
    "    \n",
    "    def __call__(self, df):\n",
    "        # 1. Set cells to None that have just white spaces\n",
    "        df = df.apply(self.white_spaces_to_None_, axis=1)\n",
    "        \n",
    "        # 2. Delete rows that have NaN values in each of its columns\n",
    "        df.dropna(axis=0, how=\"any\", subset=self.columns_name, inplace=True)\n",
    "        \n",
    "        # 3. Delete rows with class value other than [0, 1]\n",
    "        indexNames = df[~df[\"bug_class_2\"].isin(self.bug_classes)].index\n",
    "        df.drop(indexNames, axis=0, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    # set columns that just have white spaces to None\n",
    "    def white_spaces_to_None_(self, row):\n",
    "        for i in self.columns_name:\n",
    "            if row[i] and len(str(row[i]).strip()) == 0:\n",
    "                row[i] = None\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3d4d6",
   "metadata": {},
   "source": [
    "# Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "608e5c63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:41.177254Z",
     "start_time": "2022-05-16T18:25:39.928233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df_main before compose:  9201\n"
     ]
    }
   ],
   "source": [
    "df_main = pd.read_csv(\n",
    "    mypaths[\"dataset\"], \n",
    "    names=preprocessing_params[\"columns_name\"], \n",
    "    dtype=preprocessing_params[\"columns_dtype\"],\n",
    "    header=None, \n",
    "    skip_blank_lines=True\n",
    ")\n",
    "\n",
    "print(\"len df_main before compose: \", len(df_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23eacebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:42.144387Z",
     "start_time": "2022-05-16T18:25:41.184196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df_main after compose:  9201\n"
     ]
    }
   ],
   "source": [
    "composed_pre = transforms.Compose([\n",
    "    Rows(\n",
    "        preprocessing_params[\"columns_name\"], \n",
    "        preprocessing_params[\"bug_classes\"]\n",
    "    )\n",
    "])\n",
    "\n",
    "df_main = composed_pre(df_main)\n",
    "\n",
    "print(\"len df_main after compose: \", len(df_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "854cfe7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:42.192360Z",
     "start_time": "2022-05-16T18:25:42.150384Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = df_main[\"text\"].tolist()\n",
    "labels = df_main[\"bug_class_2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ed1932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:42.222343Z",
     "start_time": "2022-05-16T18:25:42.198356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(texts):   9201\n",
      "len(labels):  9201\n"
     ]
    }
   ],
   "source": [
    "print(\"len(texts):  \", len(texts))\n",
    "print(\"len(labels): \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b95114",
   "metadata": {},
   "source": [
    "# ClassDistribution\n",
    "In order to see whether dataset is `imbalance` or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d7020d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:42.270392Z",
     "start_time": "2022-05-16T18:25:42.228340Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassDistribution():\n",
    "    def __init__(self, class_distributions):\n",
    "        self.num_samples = sum(class_distributions.values())\n",
    "        self.class_distributions = class_distributions # {\"class0\": num0, \"class1\": num1, ...}\n",
    "        self.ratios = {} # {\"1/0\": ratio1, \"1/2\": ratio2, ...}\n",
    "        self.percentage = {} # {\"class0\": perc0, \"class1\": perc1, ...}\n",
    "    \n",
    "    \n",
    "    def calc_ratios(self):\n",
    "        mydata_sorted = [(key, value) for key, value in sorted(self.class_distributions.items(), \n",
    "                                                               key=lambda item: item[1], reverse=True)]\n",
    "        num_classes = len(self.class_distributions)\n",
    "        for i in range(num_classes - 1):\n",
    "            for j in range(i + 1, num_classes):\n",
    "                tempKey1 = mydata_sorted[i][0]\n",
    "                tempKey2 = mydata_sorted[j][0]\n",
    "                mykey = tempKey1 + \"/\" + tempKey2\n",
    "                \n",
    "                tempValue1 = mydata_sorted[i][1]\n",
    "                tempValue2 = mydata_sorted[j][1]\n",
    "                self.ratios[mykey] = tempValue1 / tempValue2\n",
    "    \n",
    "    def calc_percentage(self):\n",
    "        for key, value in self.class_distributions.items():\n",
    "            tempPerc = (value * 100) / self.num_samples\n",
    "            tempPerc = round(tempPerc)\n",
    "            tempPerc = str(tempPerc) + \"%\"\n",
    "            self.percentage[key] = tempPerc\n",
    "    \n",
    "    \n",
    "    def plot_data(self, dataName):\n",
    "        myclasses = list(self.class_distributions.keys())\n",
    "        num_bugreports_perclass = list(self.class_distributions.values())\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(5, 3))\n",
    "        \n",
    "        ax.bar(myclasses, num_bugreports_perclass, color =\"maroon\", label=\"Bug Reports\", width=0.4)\n",
    "        ax.set_xlabel(\"classes\")\n",
    "        ax.set_ylabel(\"Number of bug reports\")\n",
    "        ax.legend()\n",
    "        ax.set_title(\"Number of bug reports PER classes [{}]\".format(dataName))\n",
    "\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b939b",
   "metadata": {},
   "source": [
    "## obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61038192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:42.646914Z",
     "start_time": "2022-05-16T18:25:42.274394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratios     :  {'0/1': 2.0098135426889105}\n",
      "percentages:  {'0': '67%', '1': '33%'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cds = {} # {\"train\": {\"class0\": num0, \"class1\": num1, ...}, \"validation\": {...}, ...}\n",
    "# calculate class distribution for whole dataset\n",
    "temp = df_main.groupby(\"bug_class_2\").size()\n",
    "cd_whole = temp.to_dict()\n",
    "cd_whole = {str(key): value for key, value in cd_whole.items()}\n",
    "cds[\"whole\"] = cd_whole\n",
    "\n",
    "# plot class distribution for whole dataset\n",
    "wholeCD = ClassDistribution(cd_whole)\n",
    "wholeCD.calc_ratios()\n",
    "print(\"ratios     : \", wholeCD.ratios)\n",
    "wholeCD.calc_percentage()\n",
    "print(\"percentages: \", wholeCD.percentage)\n",
    "wholeCD.plot_data(\"whole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f184a2",
   "metadata": {},
   "source": [
    "# I. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dff0d6c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:25:42.709879Z",
     "start_time": "2022-05-16T18:25:42.652912Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    \n",
    "    docMaxTokenNo_org = 0\n",
    "    docMaxTokenNo_token_threshold = 0\n",
    "    my_deleted_bug = {}\n",
    "    \n",
    "    docMaxLen = 0 # max keywords allowed\n",
    "    w2vDic = {} # dic : {\"w1\": [0.1, 0.2, ...], \"w2\": [0.1, 0.3, ...], ...}\n",
    "    paddingVector = np.zeros(300, dtype=\"float32\")\n",
    "    bugRepTokens = [] # [[w1, w2, w3, ...], [w1, w2, ...], ...]\n",
    "    docMaxTokenNo = 0 # max doc len after vectorization\n",
    "    vector_tfidf = [] # array of dictinaries: [{\"w1\": 0.1, \"w2\": 0.3, ...}, {}, ...]\n",
    "    vector_em = [] # array of matrix : [ [w1Vector, w2Vector], [], ...] \n",
    "    \n",
    "    \n",
    "    def __init__(self, docMaxLen, token_threshold):\n",
    "        self.docMaxLen = docMaxLen\n",
    "        self.token_threshold = token_threshold\n",
    "    \n",
    "    \n",
    "    # tfidf of corpuses words\n",
    "    def load_tfidf(self, tfidf_path):\n",
    "        with open(tfidf_path, \"r\") as filehandle:\n",
    "            self.vector_tfidf = json.load(filehandle)\n",
    "    \n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        excludedTokens = {\"http\", \"url\", \"https\"}\n",
    "        \n",
    "        # self.df.columns[0] : \"description\"\n",
    "        for i, doc in enumerate(texts):\n",
    "            thisTokens = []\n",
    "            doc = doc.lower()\n",
    "            for token in WordPunctTokenizer().tokenize(doc):\n",
    "                if (token in string.punctuation or token in stop_words or token in excludedTokens or \n",
    "                    (not re.findall(\"\\w\", token)) or re.findall(\"\\A[0-9]\", token)):\n",
    "                    continue\n",
    "                thisTokens.append(token)\n",
    "                self.w2vDic[token] = self.paddingVector\n",
    "            if len(thisTokens) <= self.token_threshold:\n",
    "                self.bugRepTokens.append(thisTokens)\n",
    "                if (len(thisTokens) > self.docMaxTokenNo_token_threshold):\n",
    "                    self.docMaxTokenNo_token_threshold = len(thisTokens)\n",
    "            else:\n",
    "                self.my_deleted_bug[i] = len(thisTokens)\n",
    "                del labels[i]\n",
    "                del self.vector_tfidf[i]\n",
    "            if (len(thisTokens) > self.docMaxTokenNo_org):\n",
    "                self.docMaxTokenNo_org = len(thisTokens)\n",
    "    \n",
    "    \n",
    "    def loadW2V(self, w2vpath):\n",
    "         with open(w2vpath, \"rb\") as f:\n",
    "            header = f.readline()\n",
    "            model_vocab_size, model_vector_size = map(int, header.split())\n",
    "            binary_len = np.dtype(\"float32\").itemsize * model_vector_size\n",
    "            \n",
    "            for line_no in range(model_vocab_size):\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = f.read(1)\n",
    "                    if ch == b\" \":\n",
    "                        break\n",
    "                    if ch == b\"\":\n",
    "                        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "                    if ch != b\"\\n\":\n",
    "                        word.append(ch)\n",
    "                word = b\"\".join(word).decode(\"utf-8\")\n",
    "                if (word in self.w2vDic.keys()):\n",
    "                    self.w2vDic[word] = np.frombuffer(f.read(binary_len), dtype=\"float32\")\n",
    "                else:\n",
    "                    f.seek(binary_len, 1)\n",
    "    \n",
    "    \n",
    "    def vectorize_w2V (self, keywordBased=False):\n",
    "        tempVec = []\n",
    "        x = slice(0, self.docMaxLen)\n",
    "        if keywordBased:\n",
    "            print(\"keywordBased\")\n",
    "            for doc_tokens, doc_tfidf in zip(self.bugRepTokens, self.vector_tfidf):\n",
    "                # docKeywords = [m for m in doc_tfidf.keys()][x]\n",
    "                docKeywords = list(doc_tfidf.keys())[x]\n",
    "                docAbs = self.getDocAbsrtract_(doc_tokens, docKeywords)\n",
    "                tempVec = [self.w2vDic[term] for term in docAbs]\n",
    "                self.vector_em.append(tempVec)\n",
    "                if (len(tempVec) > self.docMaxTokenNo):\n",
    "                    self.docMaxTokenNo = len(tempVec)\n",
    "        else:\n",
    "            print(\"NO keywordbased\")\n",
    "            for doc_tokens in self.bugRepTokens:\n",
    "                tempVec = [self.w2vDic[term] for term in doc_tokens]\n",
    "                self.vector_em.append(tempVec)\n",
    "                if (len(tempVec) > self.docMaxTokenNo):\n",
    "                    self.docMaxTokenNo = len(tempVec)\n",
    "    \n",
    "    \n",
    "    def getDocAbsrtract_(self, doc_tok, docKeywords):\n",
    "        return [t for t in doc_tok if t in docKeywords]\n",
    "    \n",
    "    \n",
    "    def padding(self):\n",
    "        for doc in self.vector_em:\n",
    "            if (len(doc) < self.docMaxTokenNo):\n",
    "                doc.extend([self.paddingVector] * (self.docMaxTokenNo - len(doc)))\n",
    "    \n",
    "    \n",
    "    def freeMem(self):\n",
    "        self.w2vDic = {}\n",
    "        self.bugRepTokens = []\n",
    "        self.vector_tfidf = []\n",
    "        self.vector_em = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e2002",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0ae046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:27:00.980453Z",
     "start_time": "2022-05-16T18:25:42.714877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadW2V\n",
      "vectorize_w2V\n",
      "keywordBased\n"
     ]
    }
   ],
   "source": [
    "ds = Preprocessing(preprocessing_params[\"docMaxLen\"], token_threshold)\n",
    "ds.load_tfidf(mypaths[\"tfidf\"][\"output_vec\"])\n",
    "ds.tokenize(texts)\n",
    "\n",
    "# --- vectorize: w2v (keywordbased or no)\n",
    "print(\"loadW2V\")\n",
    "ds.loadW2V(mypaths[\"w2v\"][\"pre_trained_model\"])\n",
    "\n",
    "print(\"vectorize_w2V\")\n",
    "ds.vectorize_w2V(preprocessing_params[\"keyword_Based\"])\n",
    "\n",
    "ds.padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88660f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:27:01.057409Z",
     "start_time": "2022-05-16T18:27:01.004439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filter_sizes': [3, 4, 5],\n",
       " 'conv_kernel_output_channel': 64,\n",
       " 'embedding_size': 300,\n",
       " 'longest_sentence_length': 8256,\n",
       " 'dropout_zero_prob': 0.5}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params[\"longest_sentence_length\"] = ds.docMaxTokenNo\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04de851d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:27:01.202330Z",
     "start_time": "2022-05-16T18:27:01.068403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_main length:                 9201\n",
      "bugRepTokens:                   9201\n",
      "vector_tfidf:                   9201\n",
      "num neglecting bugs:            0\n",
      "my_deleted_bug:                 {}\n",
      "docMaxTokenNo_org:              8463\n",
      "docMaxTokenNo_token_threshold:  8463\n",
      "----------\n",
      "docMaxTokenNo:                  8256\n",
      "ds.w2vDic:                      27969\n",
      "ds.vector_em:                   9201\n",
      "8256\n"
     ]
    }
   ],
   "source": [
    "print(\"df_main length:                \", len(df_main))\n",
    "print(\"bugRepTokens:                  \", len(ds.bugRepTokens))\n",
    "print(\"vector_tfidf:                  \", len(ds.vector_tfidf))\n",
    "print(\"num neglecting bugs:           \", len(ds.my_deleted_bug))\n",
    "print(\"my_deleted_bug:                \", ds.my_deleted_bug)\n",
    "print(\"docMaxTokenNo_org:             \", ds.docMaxTokenNo_org)\n",
    "print(\"docMaxTokenNo_token_threshold: \", ds.docMaxTokenNo_token_threshold)\n",
    "print(\"-\" * 10)\n",
    "print(\"docMaxTokenNo:                 \", ds.docMaxTokenNo)\n",
    "print(\"ds.w2vDic:                     \", len(ds.w2vDic))\n",
    "print(\"ds.vector_em:                  \", len(ds.vector_em))\n",
    "print(len(ds.vector_em[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf99cc03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T18:27:01.242303Z",
     "start_time": "2022-05-16T18:27:01.209322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(texts):   9201\n",
      "len(labels):  9201\n"
     ]
    }
   ],
   "source": [
    "print(\"len(texts):  \", len(texts))\n",
    "print(\"len(labels): \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124e0ea",
   "metadata": {},
   "source": [
    "# II. Tensor Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219d968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:27.632330Z",
     "start_time": "2022-05-13T12:42:26.715331Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyToTensor(object):\n",
    "    def __call__(self, sample_x):\n",
    "        return torch.tensor(sample_x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3d88e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:27.709286Z",
     "start_time": "2022-05-13T12:42:27.637327Z"
    }
   },
   "outputs": [],
   "source": [
    "class AddChannelDimension(object):\n",
    "    def __call__(self, sample_x):\n",
    "        return torch.unsqueeze(sample_x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4efc3ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:27.757259Z",
     "start_time": "2022-05-13T12:42:27.714283Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDatasetTensor(Dataset):\n",
    "    \n",
    "    def __init__(self, vector_em, labels, transform=None):\n",
    "        self.x = vector_em\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "        self.len = len(self.y)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        my_x = self.x[index]\n",
    "        my_x = np.array(my_x)\n",
    "        if self.transform:\n",
    "            my_x = self.transform(my_x)\n",
    "        return my_x, self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3423f2",
   "metadata": {},
   "source": [
    "## obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de138ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:27.897177Z",
     "start_time": "2022-05-13T12:42:27.763255Z"
    }
   },
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([\n",
    "    MyToTensor(),\n",
    "    AddChannelDimension()\n",
    "])\n",
    "\n",
    "dsTen = MyDatasetTensor(ds.vector_em, labels, transform=composed_transform)\n",
    "ds.freeMem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a85f2b",
   "metadata": {},
   "source": [
    "# III. Split, Loader, Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492356e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:27.945151Z",
     "start_time": "2022-05-13T12:42:27.902176Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDatasetSplit():\n",
    "    def __init__(self, batch_size):\n",
    "        self.mykeys = [\"train\", \"validation\"]\n",
    "        self.my_datasets = {} # {\"train\": dataset0, \"validation\": dataset1}\n",
    "        self.len_datasets = {} # {\"train\": len0, \"validation\": len1}\n",
    "        self.my_dataloaders = {} # {\"train\": dataloader0, \"validation\": dataloader1}\n",
    "        self.len_dataloaders = {} # {\"train\": len0, \"validation\": len1}\n",
    "        self.batch_size = batch_size\n",
    "        self.class_weights = None # [1, 4]\n",
    "        self.sample_weights = None # [1, 1, 1, 4, 1, 4, ...]\n",
    "    \n",
    "    \n",
    "    def split_dataset(self, dataset_tensor, train_percentage):\n",
    "        dataset_tensor_len = len(dataset_tensor)\n",
    "        train_size = int(train_percentage * dataset_tensor_len)\n",
    "        validation_size = dataset_tensor_len - train_size\n",
    "        \n",
    "        rs = random_split(dataset_tensor, [train_size, validation_size])\n",
    "        \n",
    "        for i, k in enumerate(self.mykeys):\n",
    "            self.my_datasets[k] = rs[i]\n",
    "            self.len_datasets[k] = len(rs[i])\n",
    "    \n",
    "    \n",
    "    # ******************** gen_train_balanced ********************\n",
    "    \n",
    "    def calc_class_weights_(self, ratios, num_classes):\n",
    "        class_weights = [0] * num_classes\n",
    "        keys = list(ratios.keys())[:num_classes - 1]\n",
    "        max_class, _ = keys[0].split(\"/\")\n",
    "        class_weights[int(max_class)] = 1\n",
    "        for key in keys:\n",
    "            _, other_class = key.split(\"/\")\n",
    "            class_weights[int(other_class)] = round(ratios[key], 1)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    \n",
    "    def init_sample_weights_(self):\n",
    "        sample_weights = [0] * self.len_datasets[\"train\"]\n",
    "        manager = enlighten.get_manager()\n",
    "        prog = manager.counter(total=self.len_datasets[\"train\"], desc=\"init_sample_weights\", \n",
    "                               unit=\"sample\", color=\"blue\")\n",
    "        for idx, (data, label) in enumerate(self.my_datasets[\"train\"]):\n",
    "            cw = self.class_weights[label]\n",
    "            sample_weights[idx] = cw\n",
    "            prog.update()\n",
    "        self.sample_weights = sample_weights\n",
    "    \n",
    "    \n",
    "    def gen_train_balanced(self, ratios, num_classes):\n",
    "        # ---- balanced train dataloader\n",
    "        # sampler\n",
    "        self.calc_class_weights_(ratios, num_classes)\n",
    "        self.init_sample_weights_()\n",
    "        sampler = WeightedRandomSampler(self.sample_weights, num_samples=self.len_datasets[\"train\"], replacement=True)\n",
    "        # dataloader\n",
    "        self.my_dataloaders[\"train_balanced\"] = DataLoader(\n",
    "            self.my_datasets[\"train\"], \n",
    "            batch_size=self.batch_size, \n",
    "            sampler=sampler\n",
    "        )\n",
    "        self.len_dataloaders[\"train_balanced\"] = len(self.my_dataloaders[\"train_balanced\"])\n",
    "        self.len_datasets[\"train_balanced\"] = self.len_datasets[\"train\"]\n",
    "    \n",
    "    \n",
    "    # ******************** gen_dataloaders ********************\n",
    "    \n",
    "    def gen_dataloaders(self):\n",
    "        # ---- dataloaders\n",
    "        for k in self.mykeys:\n",
    "            self.my_dataloaders[k] = DataLoader(\n",
    "                self.my_datasets[k], \n",
    "                batch_size=self.batch_size, \n",
    "                shuffle=True\n",
    "            )\n",
    "            self.len_dataloaders[k] = len(self.my_dataloaders[k])\n",
    "    \n",
    "    \n",
    "    def freeMem(self):\n",
    "        self.my_datasets = None\n",
    "        self.sample_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f30c4",
   "metadata": {},
   "source": [
    "## obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b43737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:28.292904Z",
     "start_time": "2022-05-13T12:42:27.950148Z"
    }
   },
   "outputs": [],
   "source": [
    "dsTenSplit = MyDatasetSplit(dataset_params[\"batch_size\"])\n",
    "\n",
    "dsTenSplit.split_dataset(dsTen, dataset_params[\"train_size\"])\n",
    "\n",
    "if dataset_params[\"balance_train\"]:\n",
    "    print(\"-\" * 15, \"gen_train_balanced\")\n",
    "    dsTenSplit.gen_train_balanced(wholeCD.ratios, preprocessing_params[\"num_bug_classes\"])\n",
    "\n",
    "dsTenSplit.gen_dataloaders()\n",
    "dsTenSplit.freeMem()\n",
    "os.system(\"printf '\\a'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00a424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:28.320888Z",
     "start_time": "2022-05-13T12:42:28.307895Z"
    }
   },
   "outputs": [],
   "source": [
    "dsTenSplit.class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6dd8b5",
   "metadata": {},
   "source": [
    "# BatchClassDistribution\n",
    "\n",
    "- calculate `batches` class distribution for **a dataloader**\n",
    "    - and then visualize it.\n",
    "\n",
    "- calculate `total` class distribution for **a dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fe044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:28.412835Z",
     "start_time": "2022-05-13T12:42:28.331882Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchClassDistribution():\n",
    "    \n",
    "    def __init__(self, num_classes, colours):\n",
    "        # [{\"label\": \"class0\", \"values\": [num_class0inbatch0, num_class0inbatch1 , ...], \"colour\": \"colour0\"}, ...]\n",
    "        self.batch_class_distributions = [] \n",
    "        \n",
    "        # {\"class0\": num1, \"class1\": num2, ...}\n",
    "        self.class_distributions = {}\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            temp = {\n",
    "                \"label\": str(i),\n",
    "                \"values\": [],\n",
    "                \"colour\": colours[i]\n",
    "            }\n",
    "            self.batch_class_distributions.append(temp)\n",
    "            \n",
    "        self.num_classes = num_classes\n",
    "        self.len_dataloader = None\n",
    "    \n",
    "    \n",
    "    # ************************ batches class distribution for a dataloader ************************\n",
    "    \n",
    "    def calc_batch_class_distributions(self, data_loader, batch_size):\n",
    "        manager = enlighten.get_manager()\n",
    "        prog = manager.counter(total=len(data_loader), desc=\"calc_batch_class_distributions\", \n",
    "                               unit=\"batch\", color=\"gray\")\n",
    "        \n",
    "        self.len_dataloader = len(data_loader)\n",
    "        \n",
    "        maxlen = len(str(batch_size))\n",
    "        for _, y in data_loader:\n",
    "            count = Counter(y.numpy())\n",
    "            tempBCD_formated = []\n",
    "            \n",
    "            for i in range(self.num_classes):\n",
    "                tempDist = count[i] if i in count.keys() else 0\n",
    "                self.batch_class_distributions[i][\"values\"].append(tempDist)\n",
    "            \n",
    "            prog.update()\n",
    "    \n",
    "    \n",
    "    def plot_batch_class_distribution(self, dataName, start_range=None, end_range=None):\n",
    "        bar_scale = 0.8\n",
    "        \n",
    "        start_range = start_range if start_range else 0\n",
    "        end_range = end_range if end_range else self.len_dataloader\n",
    "        myindex = slice(start_range, end_range)\n",
    "        num_sample = end_range - start_range\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 4))\n",
    "        \n",
    "        for i, data in enumerate(self.batch_class_distributions):\n",
    "            # the label locations\n",
    "            temp = self.num_classes - 1\n",
    "            temp = temp / 2\n",
    "            temp = (i - temp) * bar_scale\n",
    "            temp = temp / self.num_classes\n",
    "            temp = np.arange(self.len_dataloader) + temp\n",
    "            x = 1 + temp\n",
    "            \n",
    "            ax.bar(\n",
    "                x=x[myindex],\n",
    "                height=data[\"values\"][myindex],\n",
    "                width=bar_scale / self.num_classes,\n",
    "                label=data[\"label\"],\n",
    "                color=data[\"colour\"],\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "        \n",
    "        ax.get_xaxis().set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.set_ylabel(\"Number of samples\")\n",
    "        ax.set_xlabel(\"batch numbers\")\n",
    "        ax.set_title(\"Number of class samples PER batch [{}]\".format(dataName))\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    # ************************ total class distribution for a dataloader ************************\n",
    "    \n",
    "    def calc_class_distributions(self):\n",
    "        for cl in self.batch_class_distributions:\n",
    "            temp = sum(cl[\"values\"])\n",
    "            key = cl[\"label\"]\n",
    "            self.class_distributions[key] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da13229",
   "metadata": {},
   "source": [
    "## obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e8523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:42:28.443817Z",
     "start_time": "2022-05-13T12:42:28.423829Z"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines){\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ff98c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:48:06.327730Z",
     "start_time": "2022-05-13T12:42:28.452812Z"
    }
   },
   "outputs": [],
   "source": [
    "for dataName in dsTenSplit.my_dataloaders.keys():\n",
    "    # calculate batches class distribution for a dataloader\n",
    "    print(\"-\" * 15, dataName)\n",
    "    tempBCD = BatchClassDistribution(preprocessing_params[\"num_bug_classes\"], bcd_colours)\n",
    "    tempBCD.calc_batch_class_distributions(dsTenSplit.my_dataloaders[dataName], dataset_params[\"batch_size\"])\n",
    "    \n",
    "    # plot batches class distribution for a dataloader\n",
    "    tempBCD.plot_batch_class_distribution(dataName, start_range=None, end_range=None)\n",
    "    \n",
    "    # calculate total class distribution for a dataloader\n",
    "    tempBCD.calc_class_distributions()\n",
    "    cds[dataName] = tempBCD.class_distributions\n",
    "    \n",
    "    # plot totlal class distribution for a dataloader\n",
    "    tempCDist = ClassDistribution(tempBCD.class_distributions)\n",
    "    tempCDist.calc_ratios()\n",
    "    print(\"ratios     : \", tempCDist.ratios)\n",
    "    tempCDist.calc_percentage()\n",
    "    print(\"percentages: \", tempCDist.percentage)\n",
    "    tempCDist.plot_data(dataName)\n",
    "    print(\"\\n\")\n",
    "os.system(\"printf '\\a'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ce79b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f5cc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:48:27.209092Z",
     "start_time": "2022-05-13T12:48:27.127339Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, \n",
    "                 filter_sizes, \n",
    "                 conv_kernel_output_channel, \n",
    "                 embedding_size,\n",
    "                 longest_sentence_length, \n",
    "                 num_classes, \n",
    "                 dropout_keep_prob):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # cnn and maxpool layers\n",
    "        self.cnns = nn.ModuleList()\n",
    "        self.maxpools = nn.ModuleList()\n",
    "        \n",
    "        self.cnn_list = []\n",
    "        self.maxpool_list = []\n",
    "        \n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            cnn_ = nn.Conv2d(in_channels=1, \n",
    "                             out_channels=conv_kernel_output_channel, \n",
    "                             kernel_size=(filter_size, embedding_size), \n",
    "                             stride=1, \n",
    "                             padding=0)\n",
    "            # initialize cnn weights and biases\n",
    "            torch.nn.init.trunc_normal_(cnn_.weight, std=0.1)\n",
    "            torch.nn.init.constant_(cnn_.bias, 0.1)\n",
    "            \n",
    "            maxpool_ = nn.MaxPool2d(kernel_size=(longest_sentence_length - filter_size + 1, 1), \n",
    "                                    stride=1, \n",
    "                                    padding=0)\n",
    "            self.cnns.append(cnn_)\n",
    "            self.maxpools.append(maxpool_)\n",
    "            \n",
    "        # droput layer\n",
    "        self.drop = nn.Dropout(p = dropout_keep_prob)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(conv_kernel_output_channel * len(filter_sizes), num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.constant_(self.fc1.bias, 0.1)\n",
    "        \n",
    "        self.filter_sizes = filter_sizes\n",
    "        \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.pooled_outputs = []\n",
    "        \n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            x1 = self.cnns[i](x) # convolution layer\n",
    "            x1 = torch.relu(x1) # apply activation function\n",
    "            x1 = self.maxpools[i](x1) # maxpooling layer\n",
    "            self.pooled_outputs.append(x1)\n",
    "        \n",
    "        # flatten or reshape the output\n",
    "        x1 = torch.cat(self.pooled_outputs, 1)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        \n",
    "        # dropout\n",
    "        x1 = self.drop(x1)\n",
    "        \n",
    "        # fully connected layer\n",
    "        x1 = self.fc1(x1)\n",
    "        \n",
    "        # no softmax at the end, because cross entropy loss have it implicitly\n",
    "        \n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2785d",
   "metadata": {},
   "source": [
    "# Train, Validate, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f3786",
   "metadata": {},
   "source": [
    "## MyConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c4eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:48:28.427363Z",
     "start_time": "2022-05-13T12:48:28.400379Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyConfusionMatrix():\n",
    "    def __init__(self, num_classes):\n",
    "        # rows: actual, columns: prediction\n",
    "        self.confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
    "    \n",
    "    \n",
    "    def update(self, y, yhat_indices):\n",
    "        for actual, pred in zip(y, yhat_indices):\n",
    "                self.confusion_matrix[actual, pred] += 1\n",
    "    \n",
    "    \n",
    "    def calc_accuracy(self):\n",
    "        diagon = self.confusion_matrix.diagonal()\n",
    "        # accuracy\n",
    "        total_samples = self.confusion_matrix.sum()\n",
    "        total_corrects = diagon.sum()\n",
    "        accuracy = 100 * (total_corrects / total_samples)\n",
    "        \n",
    "        # accuracy per class\n",
    "        # sum(1): 1 referes to sum for each row\n",
    "        samples_per_class = self.confusion_matrix.sum(1)\n",
    "        accuracy_per_class = 100 * (np.divide(diagon, samples_per_class))\n",
    "        \n",
    "        return accuracy, accuracy_per_class.tolist()\n",
    "    \n",
    "    \n",
    "    def get_cf(self):\n",
    "        return self.confusion_matrix.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526dd716",
   "metadata": {},
   "source": [
    "## MyTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f46a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:48:30.790355Z",
     "start_time": "2022-05-13T12:48:30.671637Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyTrain():\n",
    "    \n",
    "    def __init__(self, train_params, num_classes):\n",
    "        self.train_params = train_params\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        self.criterion = None\n",
    "        self.l2_loss = 0.0\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.decay_speed = None\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.progresses = None\n",
    "        \n",
    "        self.best_model_wts = None\n",
    "        self.useful_stuff = {\n",
    "            \"train\": {\n",
    "                \"confusion_matrix\": [], \n",
    "                \"accuracy\": [], \n",
    "                \"acc_per_class\": [], \n",
    "                \"cost\": [] \n",
    "            }, \n",
    "            \"validation\": {\n",
    "                \"confusion_matrix\": [], \n",
    "                \"accuracy\": [], \n",
    "                \"best_acc\": 0.0, \n",
    "                \"best_at_step\": 0, \n",
    "                \"acc_per_class\": [], \n",
    "                \"cost\": []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # ************************** model, criterion, optimization ************************** #\n",
    "    \n",
    "    def create_model(self, model_params):\n",
    "        self.model = CNN(\n",
    "            model_params[\"filter_sizes\"], \n",
    "            model_params[\"conv_kernel_output_channel\"], \n",
    "            model_params[\"embedding_size\"], \n",
    "            model_params[\"longest_sentence_length\"], \n",
    "            self.num_classes, \n",
    "            model_params[\"dropout_zero_prob\"])\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Available\")\n",
    "            self.model = self.model.cuda()\n",
    "        print(\"my_model: \", self.model)\n",
    "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "    \n",
    "    \n",
    "    def create_criterion_optimization(self):\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.train_params[\"max_learning_rate\"])\n",
    "    \n",
    "    \n",
    "    # ************************** learning rate ************************** #\n",
    "    \n",
    "    def lr_init_decay_speed(self, batch_size, len_dataset_train):\n",
    "        self.decay_speed = self.train_params[\"decay_coefficient\"] * len_dataset_train / batch_size\n",
    "    \n",
    "    \n",
    "    def lr_update_(self):\n",
    "        # compute learning rate for each batch.\n",
    "        # max value of counter is: epoch * number of batches\n",
    "        # self.min_learning_rate + (self.max_learning_rate - self.min_learning_rate) * math.exp(-counter / self.decay_speed)\n",
    "        temp_lr = (self.train_params[\"max_learning_rate\"] - self.train_params[\"min_learning_rate\"])\n",
    "        temp_lr = temp_lr * math.exp(-self.counter / self.decay_speed)\n",
    "        temp_lr = temp_lr + self.train_params[\"min_learning_rate\"]\n",
    "        self.counter += 1\n",
    "        return temp_lr\n",
    "    \n",
    "    \n",
    "    # ************************** progress lines ************************** #\n",
    "    \n",
    "    def progress_lines(self, len_dataloaders, colours):\n",
    "        total = [self.train_params[\"epochs\"], len_dataloaders[\"train\"], len_dataloaders[\"validation\"]]\n",
    "        desc = [\"epoch\", \"train\", \"validation\"]\n",
    "        unit = [\"epoch\", \"batch\", \"batch\"]\n",
    "        temp_desc = self.set_strings_to_equal_len_(desc)\n",
    "        unit = self.set_strings_to_equal_len_(unit)\n",
    "        \n",
    "        manager = enlighten.get_manager()\n",
    "        progresses = {}\n",
    "        for i in range(len(total)):\n",
    "            prog = manager.counter(total=total[i], desc=temp_desc[i], unit=unit[i], color=colours[i])\n",
    "            progresses[desc[i]] = prog\n",
    "        self.progresses = progresses\n",
    "    \n",
    "    \n",
    "    def set_strings_to_equal_len_(self, words):\n",
    "        max_len = 0\n",
    "        longest_string_length = len(max(words, key=len))\n",
    "        w = []\n",
    "        for i, word in enumerate(words):\n",
    "            temp = longest_string_length - len(word)\n",
    "            w.append(word + \" \" * temp)\n",
    "        return w\n",
    "    \n",
    "    \n",
    "    # ************************** epoch ************************** #\n",
    "    \n",
    "    def epoch_train_validate(self, train_dataloader, validation_dataloader):\n",
    "        for epoch in range(self.train_params[\"epochs\"]):\n",
    "            self.progresses[\"train\"].refresh()\n",
    "            self.progresses[\"validation\"].refresh()\n",
    "            \n",
    "            self.train_model_(train_dataloader)\n",
    "            self.validate_model_(validation_dataloader, epoch)\n",
    "            \n",
    "            self.progresses[\"epoch\"].update()\n",
    "        \n",
    "        # load best model weights\n",
    "        self.model.load_state_dict(self.best_model_wts)\n",
    "    \n",
    "    # ================================================================== #\n",
    "    #                            Train section                           #\n",
    "    # ================================================================== #\n",
    "    \n",
    "    def train_model_(self, train_dataloader):\n",
    "        self.model.train() # Set model to training mode\n",
    "        cost = 0.0\n",
    "        # rows: actual, columns: prediction\n",
    "        confusion_matrix = MyConfusionMatrix(self.num_classes)\n",
    "        \n",
    "        for x, y in train_dataloader:\n",
    "            if torch.cuda.is_available(): # Transfer Data to GPU if available\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "            # forward / prediction\n",
    "            yhat = self.model(x) # make a prediction\n",
    "            loss = self.criterion(yhat, y) # calculate loss\n",
    "            \n",
    "            # L2-regularization\n",
    "            # just apply on the last layer (Linear layer) of the cnn model \n",
    "            my_w = self.model.fc1.weight\n",
    "            my_b = self.model.fc1.bias\n",
    "            self.l2_loss = 0.0\n",
    "            self.l2_loss += torch.sum(my_w ** 2) / 2\n",
    "            self.l2_loss += torch.sum(my_b ** 2) / 2\n",
    "            loss = loss + self.train_params[\"l2_reg_lambda\"] * self.l2_loss\n",
    "            \n",
    "            # backwards\n",
    "            self.optimizer.zero_grad() # clear gradient\n",
    "            self.optimizer.param_groups[0][\"lr\"] = self.lr_update_() # update learning rate\n",
    "            loss.backward() # calculate gradients of parameters\n",
    "            self.optimizer.step() # update parameters\n",
    "            \n",
    "            # accumulate loss and num_correct\n",
    "            # we refer to \"accumulated loss\" as \"cost\"\n",
    "            cost += loss.item()\n",
    "            \n",
    "            _, yhat_indices = torch.max(yhat.data, 1)\n",
    "            confusion_matrix.update(y, yhat_indices)\n",
    "            \n",
    "            self.progresses[\"train\"].update()\n",
    "        # ----- end for\n",
    "        \n",
    "        # save cost, accuracy\n",
    "        train_accuracy, acc_per = confusion_matrix.calc_accuracy()\n",
    "        self.useful_stuff[\"train\"][\"confusion_matrix\"].append(confusion_matrix.get_cf())\n",
    "        self.useful_stuff[\"train\"][\"accuracy\"].append(train_accuracy)\n",
    "        self.useful_stuff[\"train\"][\"acc_per_class\"].append(acc_per)\n",
    "        self.useful_stuff[\"train\"][\"cost\"].append(cost)\n",
    "        self.progresses[\"train\"].count = 0\n",
    "    \n",
    "    # ================================================================== #\n",
    "    #                          Validate section                          #\n",
    "    # ================================================================== #\n",
    "    \n",
    "    def validate_model_(self, validation_dataloader, epoch):\n",
    "        self.model.eval() # Set model to evaluate mode\n",
    "        cost = 0.0\n",
    "        confusion_matrix = MyConfusionMatrix(self.num_classes)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_dataloader:\n",
    "                if torch.cuda.is_available():\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "                # prediction\n",
    "                yhat = self.model(x)\n",
    "                loss = self.criterion(yhat, y)\n",
    "\n",
    "                # accumulate loss and Confusion Matrix\n",
    "                cost += loss.item()\n",
    "                \n",
    "                _, yhat_indices = torch.max(yhat.data, 1)\n",
    "                confusion_matrix.update(y, yhat_indices)\n",
    "                \n",
    "                self.progresses[\"validation\"].update()\n",
    "            # ----- end for\n",
    "            \n",
    "            # save cost, accuracy and model\n",
    "            val_accuracy, acc_per = confusion_matrix.calc_accuracy()\n",
    "            if val_accuracy > self.useful_stuff[\"validation\"][\"best_acc\"]:\n",
    "                self.useful_stuff[\"validation\"][\"best_acc\"] = val_accuracy\n",
    "                self.useful_stuff[\"validation\"][\"best_at_step\"] = epoch + 1\n",
    "                self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "            \n",
    "            self.useful_stuff[\"validation\"][\"confusion_matrix\"].append(confusion_matrix.get_cf())\n",
    "            self.useful_stuff[\"validation\"][\"accuracy\"].append(val_accuracy)\n",
    "            self.useful_stuff[\"validation\"][\"acc_per_class\"].append(acc_per)\n",
    "            self.useful_stuff[\"validation\"][\"cost\"].append(cost)\n",
    "            self.progresses[\"validation\"].count = 0\n",
    "    \n",
    "    \n",
    "    # ************************** save ************************** #\n",
    "    \n",
    "    def save_to_file_model(self, model_path):\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "    \n",
    "    \n",
    "    def save_to_file_results(self, dataset_name, preprocessing_params, dataset_params, class_distributions, \n",
    "                             model_params, result_path, ):\n",
    "        tempStructure = {\n",
    "            \"dataset\": dataset_name,\n",
    "            \"preprocessing_params\": preprocessing_params,\n",
    "            \"dataset_params\": dataset_params,\n",
    "            \"model_params\": model_params,\n",
    "            \"train_params\": self.train_params,\n",
    "            \"class_distributions\": class_distributions,\n",
    "            \"model_results\": self.useful_stuff\n",
    "        }\n",
    "        \n",
    "        # time to train\n",
    "        totalSeconds = self.progresses[\"epoch\"].elapsed\n",
    "        hours = int(totalSeconds / 3600)\n",
    "        minutes = int((totalSeconds - (hours * 3600)) / 60)\n",
    "        seconds = int((totalSeconds - (hours * 3600 + minutes * 60)))\n",
    "        tempStructure[\"model_results\"][\"time_to_train\"] = f\"{hours}h:{minutes}m:{seconds}s\"\n",
    "        \n",
    "        with open(result_path, \"w\") as fout:\n",
    "            json.dump(tempStructure, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183e8c4",
   "metadata": {},
   "source": [
    "## flow_train_on_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240a872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:48:59.124280Z",
     "start_time": "2022-05-13T12:48:59.104293Z"
    }
   },
   "outputs": [],
   "source": [
    "def flow_train_on_data(dataName, file_subversion):\n",
    "    mtr = MyTrain(train_params, preprocessing_params[\"num_bug_classes\"])\n",
    "\n",
    "    # -------- model, criterion, optimization\n",
    "    print(\"*\" * 15, \"model, criterion, optimization\")\n",
    "    mtr.create_model(model_params)\n",
    "    mtr.create_criterion_optimization()\n",
    "\n",
    "    # -------- learning rate\n",
    "    print(\"*\" * 15, \"lr_init_decay_speed\")\n",
    "    mtr.lr_init_decay_speed(\n",
    "        dataset_params[\"batch_size\"], \n",
    "        dsTenSplit.len_datasets[\"train\"]\n",
    "    )\n",
    "\n",
    "    # -------- progress lines\n",
    "    print(\"*\" * 15, \"train on\", file_subversion)\n",
    "    mtr.progress_lines(\n",
    "        dsTenSplit.len_dataloaders, \n",
    "        [\"black\", \"blue\", \"gray\", \"lightskyblue1\"]\n",
    "    )\n",
    "\n",
    "    # -------- epoch\n",
    "    mtr.epoch_train_validate(\n",
    "        dsTenSplit.my_dataloaders[dataName], \n",
    "        dsTenSplit.my_dataloaders[\"validation\"]\n",
    "    )\n",
    "    \n",
    "    # -------- save\n",
    "    if preprocessing_params[\"keyword_Based\"]:\n",
    "        prefix = \"tfidf\"\n",
    "    else:\n",
    "        prefix = \"w2v\"\n",
    "    # model\n",
    "    model_path = mypaths[prefix][\"output_model\"].format(file_subversion)\n",
    "    mtr.save_to_file_model(model_path)\n",
    "    \n",
    "    # result\n",
    "    result_path = mypaths[prefix][\"output_performance\"].format(file_subversion)\n",
    "    mtr.save_to_file_results(mypaths[\"dataset\"], \n",
    "                             preprocessing_params, \n",
    "                             dataset_params, \n",
    "                             cds, \n",
    "                             model_params, \n",
    "                             result_path)\n",
    "    # return mtr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6cbe33",
   "metadata": {},
   "source": [
    "## call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655838eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:49:06.547953Z",
     "start_time": "2022-05-13T12:48:59.760126Z"
    }
   },
   "outputs": [],
   "source": [
    "# train on imbalance data\n",
    "flow_train_on_data(\"train\", mypaths[\"file_subversion\"][\"im\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e423c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:20:54.908531Z",
     "start_time": "2022-05-13T12:20:54.908531Z"
    }
   },
   "outputs": [],
   "source": [
    "# train on balanced data\n",
    "if \"train_balanced\" in dsTenSplit.my_dataloaders:\n",
    "    flow_train_on_data(\"train_balanced\", mypaths[\"file_subversion\"][\"ba\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfb371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-pytorch-cpu_jn",
   "language": "python",
   "name": "paper-pytorch-cpu_jn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "190px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
